{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Modal Video Search with TwelveLabs and Pixeltable\n",
    "\n",
    "TwelveLabs' [Marengo](https://docs.twelvelabs.io/docs/marengo) model projects text, images, audio, and video into the same semantic space. That means a text description, a still frame, an audio clip, and a video segment of the same moment all land near each other in the embedding space — and you can search across all of them with a single index.\n",
    "\n",
    "| Query Type | Use Case |\n",
    "|------------|----------|\n",
    "| **Text → Video** | Find clips matching \"a man giving a speech\" |\n",
    "| **Image → Video** | Find videos visually similar to a reference photo |\n",
    "| **Audio → Video** | Find videos with similar speech or sounds |\n",
    "| **Video → Video** | Find similar clips or alternative takes |\n",
    "\n",
    "This notebook walks through all four, then shows how the same embeddings apply to non-video data like text and images.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- A TwelveLabs API key from [playground.twelvelabs.io](https://playground.twelvelabs.io/)\n",
    "- Audio and video content must be at least 4 seconds long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU pixeltable twelvelabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/alison-pxt/.pixeltable/pgdata\n",
      "Created directory 'twelvelabs_demo'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alison-pxt/Documents/Github/mm-demo/.venv/lib/python3.12/site-packages/pixeltable/env.py:494: UserWarning: Progress reporting is disabled because ipywidgets is not installed. To fix this, run: `pip install ipywidgets`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pixeltable.catalog.dir.Dir at 0x1226c46b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pixeltable as pxt\n",
    "import pixeltable.functions as pxtf\n",
    "\n",
    "pxt.drop_dir('twelvelabs_demo', force=True)\n",
    "pxt.create_dir('twelvelabs_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tlk_0VPEVXA09GTH5C2ER0NRC2H875YS\n"
     ]
    }
   ],
   "source": [
    "from pixeltable.config import Config\n",
    "print(Config.get().get_value('api_key', str, section='twelvelabs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will work 100% of the time\n",
    "import os\n",
    "os.environ['TWELVELABS_API_KEY'] = 'tlk_20YHK7S34ZGH3J23267QT2MCZ5MR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Modal Video Search\n",
    "\n",
    "The workflow has three steps: create a table, split the video into segments, and build an embedding index. After that, every search modality works through the same `.similarity()` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a video table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table for videos\n",
    "half = pxt.create_table('twelvelabs_demo.halftime_show', {'video': pxt.Video})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an empty table with a schema for now. In this notebook, we'll revisit the table schema as we build up our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can insert many rows of video files at this point into the table - videos can be referenced wherever they are actually stored:\n",
    "\n",
    "- Local paths (this example)\n",
    "- HTTP URLs\n",
    "- Cloud storage (like S3, GCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a sample video\n",
    "video_url = 'media/vid-lx-halftime-show.mp4'\n",
    "half.insert([{'video': video_url}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pixeltable, we use `collect()` to view a table's contents. You can also use `head(n)` or `tail(n)` for bigger tables to print just a few rows. You could also combine `limit(n).collect()`. But here we have just 1 video in our dataset to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo notebook, I'll just create a clip of the first 5 minutes using a built-in UDF in Pixeltable for clipping a video with the start time and duration that I want. Pixeltable includes built-in UDFs for every media type - you can use them in computed columns like `pxtf.<type>.<udf>`. See the docs here: https://docs.pixeltable.com/sdk/latest/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a computed column that clips the first 5 minutes\n",
    "half.add_computed_column(\n",
    "    first_five=pxtf.video.clip(half.video, start_time=0.0, duration=300.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the video into segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Views are a core concept in Pixeltable, and in addition to computed columns, are another way to add a different level or orchestration to your table. \n",
    "\n",
    "Views are powerful on their own, as they allow you to create a derived table on top of a base table. That table could be filtered by a column value, or some other subset of rows, so you can quickly experiment with expensive operations on a subset of your dataset.\n",
    "\n",
    "Pixeltable also provides a special kind of view called an iterator view, which we'll use here. It enables you to essentially expand your base table. Imagine a view that chunks a document, splits text by sentences, chops audio files up into clips, or here, we'll split a video into 15-second segments. You can get pretty creative here and even split videos into scenes based on scene detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into segments (TwelveLabs requires at least 4 seconds)\n",
    "video_chunks = pxt.create_view(\n",
    "    'twelvelabs_demo.video_chunks',\n",
    "    half,\n",
    "    iterator=pxtf.video.video_splitter(\n",
    "        video=half.first_five,          # note that I'm using the computed column here\n",
    "        duration=15.0, \n",
    "        min_segment_duration=4.0\n",
    "    ),\n",
    "    if_exists='replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_chunks.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can peek at the first 5 rows. When we create an iterator view like this, you can see that Pixeltable does an implicit join on the parent table, so we have the video and first five minute clip on every row. This data is not duplicated though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_chunks.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the scheme for our iterator view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the embedding index using TwelveLabs Marengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_chunks.add_embedding_index(\n",
    "    video_chunks.video_segment,\n",
    "    embedding=pxtf.twelvelabs.embed.using(model_name='marengo3.0'),\n",
    "    if_exists='replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for setup. Pixeltable sent each segment to the TwelveLabs Embed API, stored the 512-dimensional vectors, and built a searchable index. When new videos are inserted later, the index updates incrementally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-modal video search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search #1: Text to Video\n",
    "\n",
    "The `.similarity()` method on an indexed column returns an expression that works in `order_by`, `select`, and `where` clauses. For text queries, pass the `string=` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = video_chunks.video_segment.similarity(string='a man giving a speech')\n",
    "\n",
    "video_chunks.order_by(sim, asc=False).limit(3).select(\n",
    "    video_chunks.video_segment, score=sim\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search #2: Image to Video\n",
    "\n",
    "The same index handles image queries. Here we use a screenshot from the video as the reference — Pixeltable finds the segments that are visually closest to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_query = 'https://github.com/pixeltable/pixeltable/raw/main/docs/resources/The-Pursuit-of-Happiness-Screenshot.png'\n",
    "\n",
    "sim = video_chunks.video_segment.similarity(image=image_query)\n",
    "\n",
    "video_chunks.order_by(sim, asc=False).limit(3).select(\n",
    "    video_chunks.video_segment, score=sim\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search #3: Audio to Video\n",
    "\n",
    "An audio clip works the same way — useful for matching speech patterns, background music, or ambient sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_query = 'https://github.com/pixeltable/pixeltable/raw/main/docs/resources/The-Pursuit-of-Happiness-Audio-Extract.m4a'\n",
    "\n",
    "sim = video_chunks.video_segment.similarity(audio=audio_query)\n",
    "\n",
    "video_chunks.order_by(sim, asc=False).limit(3).select(\n",
    "    video_chunks.video_segment, score=sim\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search #4: Video to Video\n",
    "\n",
    "A video segment or clip can also be the query. This is the basis for recommendations, duplicate detection, or finding alternative takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_query = 'https://github.com/pixeltable/pixeltable/raw/main/docs/resources/The-Pursuit-of-Happiness-Video-Extract.mp4'\n",
    "\n",
    "sim = video_chunks.video_segment.similarity(video=video_query)\n",
    "\n",
    "video_chunks.order_by(sim, asc=False).limit(3).select(\n",
    "    video_chunks.video_segment, score=sim\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrowing the Embedding Context\n",
    "\n",
    "For video embeddings, Marengo can focus on specific aspects of the content:\n",
    "\n",
    "- `'visual'` — what you see\n",
    "- `'audio'` — what you hear\n",
    "- `'transcription'` — what is said\n",
    "\n",
    "By default, Marengo uses all three. If your use case only cares about one aspect — say, matching visual style regardless of dialogue — you can narrow the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a visual-only embedding as a computed column\n",
    "video_chunks.add_computed_column(\n",
    "    visual_embedding=pxtf.twelvelabs.embed(\n",
    "        video_chunks.video_segment,\n",
    "        model_name='marengo3.0',\n",
    "        embedding_option=['visual'],\n",
    "    )\n",
    ")\n",
    "\n",
    "video_chunks.select(\n",
    "    video_chunks.video_segment, video_chunks.visual_embedding\n",
    ").limit(2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Video: Text, Images, and Documents\n",
    "\n",
    "TwelveLabs embeddings aren't limited to video. The same Marengo model embeds text and images, which means you can build a single table with multiple embedding indexes — one per column — and search across modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A content table with text and image columns\n",
    "content = pxt.create_table(\n",
    "    'twelvelabs_demo.content',\n",
    "    {\n",
    "        'title': pxt.String,\n",
    "        'description': pxt.String,\n",
    "        'thumbnail': pxt.Image,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Embedding index on the text column\n",
    "content.add_embedding_index(\n",
    "    idx_name='desc_idx',\n",
    "    column=content.description, \n",
    "    embedding=pxtf.twelvelabs.embed.using(model_name='marengo3.0')\n",
    ")\n",
    "\n",
    "# Embedding index on the image column\n",
    "content.add_embedding_index(\n",
    "    idx_name='thumb_idx',\n",
    "    column=content.thumbnail, \n",
    "    embedding=pxtf.twelvelabs.embed.using(model_name='marengo3.0')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.insert([\n",
    "    {\n",
    "        'title': 'Beach Sunset',\n",
    "        'description': 'A beautiful sunset over the ocean with palm trees.',\n",
    "        'thumbnail': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000025.jpg',\n",
    "    },\n",
    "    {\n",
    "        'title': 'Mountain Hiking',\n",
    "        'description': 'Hikers climbing a steep mountain trail with scenic views.',\n",
    "        'thumbnail': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000139.jpg',\n",
    "    },\n",
    "    {\n",
    "        'title': 'City Street',\n",
    "        'description': 'Busy urban street with cars and pedestrians.',\n",
    "        'thumbnail': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000042.jpg',\n",
    "    },\n",
    "    {\n",
    "        'title': 'Wildlife Safari',\n",
    "        'description': 'Elephants and zebras on the African savanna.',\n",
    "        'thumbnail': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000061.jpg',\n",
    "    },\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search by text description\n",
    "sim = content.description.similarity(string='outdoor nature adventure')\n",
    "\n",
    "content.order_by(sim, asc=False).limit(2).select(\n",
    "    content.title, content.description, score=sim\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search by image similarity\n",
    "query_image = 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000001.jpg'\n",
    "\n",
    "sim = content.thumbnail.similarity(image=query_image)\n",
    "\n",
    "content.order_by(sim, asc=False).limit(2).select(\n",
    "    content.title, content.thumbnail, score=sim\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-modal: search images using text\n",
    "sim = content.thumbnail.similarity(string='animals in the wild')\n",
    "\n",
    "content.order_by(sim, asc=False).limit(2).select(\n",
    "    content.title, content.thumbnail, score=sim\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That last query is worth pausing on. The embedding index was built on an image column, but the search query is text. Because Marengo puts both modalities in the same vector space, this works without any extra configuration — no separate text model, no alignment step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The pattern is the same throughout: create a table, add an embedding index, and search with `.similarity()`. The query modality — text, image, audio, or video — is an argument, not a different API.\n",
    "\n",
    "A few things to keep in mind:\n",
    "\n",
    "- **Embedding options** let you focus on visual, audio, or transcription aspects of video content\n",
    "- **Multiple indexes per table** work on different columns (text, image, etc.) with the same model\n",
    "- **Incremental updates** happen automatically — inserting new rows recomputes only the new embeddings\n",
    "- **Similarity thresholds** in `.where()` clauses help control precision vs. recall\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- [TwelveLabs Documentation](https://docs.twelvelabs.io/)\n",
    "- [Pixeltable Documentation](https://docs.pixeltable.com/)\n",
    "- [Pixeltable GitHub](https://github.com/pixeltable/pixeltable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
